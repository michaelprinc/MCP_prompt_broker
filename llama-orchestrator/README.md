# llama-orchestrator

> Docker-like CLI orchestration for llama.cpp server instances on Windows

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## Overview

**llama-orchestrator** is a Python-based control plane for managing multiple llama.cpp server instances. It provides:

- ğŸš€ **Multi-instance support** â€” Run multiple models on different ports
- ğŸ”„ **Health monitoring** â€” Automatic health checks with configurable policies
- â™»ï¸ **Auto-restart** â€” Intelligent restart on failure with exponential backoff
- ğŸ“Š **TUI Dashboard** â€” Live terminal dashboard showing all instances
- ğŸªŸ **Windows native** â€” Task Scheduler / NSSM service integration

## Quick Start

```powershell
# Install
pip install -e .

# Create instance config
llama-orch init gpt-oss --model ../models/gpt-oss-20b-Q4_K_S.gguf --port 8001

# Start instance
llama-orch up gpt-oss

# Check status
llama-orch ps

# View dashboard
llama-orch dashboard

# Stop instance
llama-orch down gpt-oss
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTROL PLANE (Python)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   CLI   â”‚â”€â”€â”€â–¶â”‚ Daemon  â”‚â”€â”€â”€â–¶â”‚   TUI   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚       â”‚              â”‚              â”‚                       â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                      â–¼                                      â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚            â”‚  State (SQLite) â”‚                             â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PLANE (llama.cpp)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ :8001   â”‚    â”‚ :8002   â”‚    â”‚ :8003   â”‚                 â”‚
â”‚  â”‚ model-A â”‚    â”‚ model-B â”‚    â”‚ model-C â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## CLI Commands

| Command | Description |
|---------|-------------|
| `llama-orch up <name>` | Start an instance |
| `llama-orch down <name>` | Stop an instance |
| `llama-orch restart <name>` | Restart an instance |
| `llama-orch ps` | List all instances |
| `llama-orch health <name>` | Check instance health |
| `llama-orch logs <name>` | View instance logs |
| `llama-orch describe <name>` | Show full config + status |
| `llama-orch dashboard` | Live TUI dashboard |
| `llama-orch config validate` | Validate configuration |
| `llama-orch daemon start` | Start background daemon |

## Configuration

Instance configs are stored in `instances/<name>/config.json`:

```json
{
  "name": "gpt-oss",
  "model": {
    "path": "../../models/gpt-oss-20b-Q4_K_S.gguf",
    "context_size": 4096,
    "batch_size": 512,
    "threads": 16
  },
  "server": {
    "host": "127.0.0.1",
    "port": 8001,
    "parallel": 4
  },
  "gpu": {
    "backend": "vulkan",
    "device_id": 1,
    "layers": 30
  },
  "healthcheck": {
    "interval": 10,
    "timeout": 5,
    "retries": 3
  },
  "restart_policy": {
    "enabled": true,
    "max_retries": 5
  }
}
```

## Directory Structure

```
llama-orchestrator/
â”œâ”€â”€ bin/llama-server.exe      # llama.cpp binary
â”œâ”€â”€ instances/                 # Instance configurations
â”‚   â””â”€â”€ <name>/config.json
â”œâ”€â”€ state/state.sqlite        # Runtime state
â”œâ”€â”€ logs/<name>/              # Instance logs
â”‚   â”œâ”€â”€ stdout.log
â”‚   â””â”€â”€ stderr.log
â””â”€â”€ src/llama_orchestrator/   # Python package
```

## Requirements

- Python 3.11+
- Windows 10/11
- llama.cpp server binary (Vulkan/CPU)
- AMD GPU with Vulkan support (optional)

## Development

```powershell
# Clone and setup
git clone <repo>
cd llama-orchestrator
uv sync

# Run tests
pytest

# Run in dev mode
python -m llama_orchestrator --help
```

## Documentation

- [Implementation Plan](docs/IMPLEMENTATION_PLAN.md)
- [Implementation Checklist](docs/CHECKLIST.md)

## License

MIT
