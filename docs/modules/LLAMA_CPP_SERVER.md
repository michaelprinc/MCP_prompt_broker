# Llama CPP Server - TechnickÃ¡ dokumentace modulu

> **Verze dokumentace:** 1.0.0  
> **Datum:** 31. prosince 2025  
> **ÃšroveÅˆ:** 3/4 - Module Technical Documentation

---

## ğŸ“‹ Obsah

1. [PÅ™ehled modulu](#pÅ™ehled-modulu)
2. [Struktura adresÃ¡Å™Å¯](#struktura-adresÃ¡Å™Å¯)
3. [Konfigurace serveru](#konfigurace-serveru)
4. [GPU akcelerace](#gpu-akcelerace)
5. [API endpointy](#api-endpointy)
6. [SpouÅ¡tÄ›nÃ­ a sprÃ¡va](#spouÅ¡tÄ›nÃ­-a-sprÃ¡va)
7. [Integrace s llama-orchestrator](#integrace-s-llama-orchestrator)
8. [Troubleshooting](#troubleshooting)

---

## PÅ™ehled modulu

**Llama CPP Server** je konfiguraÄnÃ­ wrapper pro llama.cpp inference server s optimalizacÃ­ pro AMD GPU na Windows.

### TechnickÃ© charakteristiky

| Vlastnost | Hodnota |
|-----------|---------|
| **Backend** | llama.cpp |
| **GPU Backend** | Vulkan (AMD RDNA 2) |
| **API** | OpenAI-compatible |
| **Port** | 8001 (default) |
| **Model** | GPT-OSS-20B Q4_K_S |
| **VÃ½kon** | ~12 t/s (CPU), ~62 t/s (GPU)* |

*\* Pro standardnÃ­ modely. GPT-OSS mÃ¡ specifickÃ© mxfp4 tensory.*

### AktuÃ¡lnÃ­ stav

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           CURRENT STATUS                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  âœ… Status: FUNKÄŒNÃ                                                         â”‚
â”‚                                                                             â”‚
â”‚  Model:     GPT-OSS-20B Q4_K_S (10.81 GB)                                  â”‚
â”‚  Port:      8001                                                            â”‚
â”‚  Backend:   Vulkan                                                          â”‚
â”‚  Mode:      CPU inference (mxfp4 tensory nemajÃ­ GPU podporu)               â”‚
â”‚                                                                             â”‚
â”‚  Performance:                                                               â”‚
â”‚  â”œâ”€â”€ Generation: ~12 tokens/s                                               â”‚
â”‚  â”œâ”€â”€ Prompt:     ~17 tokens/s                                               â”‚
â”‚  â””â”€â”€ Context:    4096 tokens                                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Struktura adresÃ¡Å™Å¯

```
llama-cpp-server/
â”œâ”€â”€ README.md               # Dokumentace modulu
â”œâ”€â”€ config.json             # Konfigurace serveru
â”œâ”€â”€ start-server.ps1        # SpouÅ¡tÄ›cÃ­ PowerShell skript
â”œâ”€â”€ test-api.ps1            # TestovacÃ­ skript API
â”œâ”€â”€ bin/
â”‚   â”œâ”€â”€ llama-server.exe    # llama.cpp server binary
â”‚   â”œâ”€â”€ LICENSE-curl        # Licence zÃ¡vislostÃ­
â”‚   â”œâ”€â”€ LICENSE-httplib
â”‚   â”œâ”€â”€ LICENSE-jsonhpp
â”‚   â””â”€â”€ LICENSE-linenoise
â””â”€â”€ (models/)               # Symlink nebo parent folder s modely
    â””â”€â”€ gpt-oss-20b-Q4_K_S.gguf
```

---

## Konfigurace serveru

### config.json

```json
{
    "server": {
        "host": "127.0.0.1",
        "port": 8001,
        "parallel": 4
    },
    "model": {
        "path": "../models/gpt-oss-20b-Q4_K_S.gguf",
        "context_size": 4096,
        "batch_size": 512,
        "gpu_layers": 0
    },
    "inference": {
        "threads": 16,
        "flash_attention": false,
        "rope_frequency_base": 10000,
        "rope_frequency_scale": 1.0
    },
    "logging": {
        "level": "info",
        "file": null
    }
}
```

### KonfiguraÄnÃ­ parametry

| Sekce | Parametr | Typ | Default | Popis |
|-------|----------|-----|---------|-------|
| server | host | string | 127.0.0.1 | Listen address |
| server | port | int | 8001 | Listen port |
| server | parallel | int | 4 | Max parallel requests |
| model | path | string | - | Cesta k GGUF modelu |
| model | context_size | int | 4096 | Context window |
| model | batch_size | int | 512 | Batch size |
| model | gpu_layers | int | 0 | GPU offload layers |
| inference | threads | int | 8 | CPU threads |
| inference | flash_attention | bool | false | Flash attention |

---

## GPU akcelerace

### Backend srovnÃ¡nÃ­

| Backend | VÃ½kon | SloÅ¾itost | Windows | PoznÃ¡mka |
|---------|-------|-----------|---------|----------|
| **Vulkan** âœ“ | ~62 t/s | SnadnÃ¡ | âœ… | DoporuÄeno |
| ROCm/HIP | ~96 t/s | SloÅ¾itÃ¡ | âŒ | Pouze Linux |
| CUDA | ~100 t/s | StÅ™ednÃ­ | âœ… | Pouze NVIDIA |
| CPU | ~12 t/s | SnadnÃ¡ | âœ… | Fallback |

### Vulkan setup

```powershell
# Vulkan je nativnÄ› podporovÃ¡n ve Windows s AMD GPU
# NenÃ­ potÅ™eba Å¾Ã¡dnÃ¡ dodateÄnÃ¡ instalace

# OvÄ›Å™enÃ­ Vulkan podpory
vulkaninfo --summary

# llama.cpp automaticky detekuje Vulkan
.\bin\llama-server.exe --help | Select-String gpu
```

### GPU Layers konfigurace

```powershell
# StandardnÃ­ modely (LLaMA, Mistral, Qwen)
.\start-server.ps1 -GpuLayers 99  # VÅ¡echny vrstvy na GPU

# GPT-OSS (mxfp4 tensory - nenÃ­ GPU podpora)
.\start-server.ps1 -GpuLayers 0   # CPU only
```

### ProÄ CPU pro GPT-OSS?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GPT-OSS TENSOR FORMAT                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  GPT-OSS-20B pouÅ¾Ã­vÃ¡ Microsoft FP4 (mxfp4) tensor format:                  â”‚
â”‚                                                                             â”‚
â”‚  Standard GGUF:     Q4_K_S, Q5_K_M, Q8_0, F16, F32                         â”‚
â”‚                           â†“                                                 â”‚
â”‚                     Full GPU support via Vulkan/CUDA                        â”‚
â”‚                                                                             â”‚
â”‚  GPT-OSS:           mxfp4 (Microsoft FP4)                                  â”‚
â”‚                           â†“                                                 â”‚
â”‚                     Limited GPU support (work in progress)                  â”‚
â”‚                           â†“                                                 â”‚
â”‚                     Falls back to CPU inference                             â”‚
â”‚                                                                             â”‚
â”‚  DÅ¯sledek: ~12 t/s mÃ­sto ~62 t/s, ale stÃ¡le pouÅ¾itelnÃ©                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## API endpointy

### OpenAI-Compatible API

| Endpoint | Method | Popis |
|----------|--------|-------|
| `/` | GET | Web UI interface |
| `/health` | GET | Health check |
| `/v1/chat/completions` | POST | Chat completions |
| `/v1/completions` | POST | Text completions |
| `/v1/embeddings` | POST | Text embeddings |
| `/v1/models` | GET | List models |

### Chat Completions

```bash
# Request
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-oss-20b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'

# Response
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1735654800,
  "model": "gpt-oss-20b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 8,
    "total_tokens": 23
  }
}
```

### Streaming

```bash
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-oss-20b",
    "messages": [{"role": "user", "content": "Tell me a joke"}],
    "stream": true
  }'
```

### Health Check

```bash
curl http://localhost:8001/health

# Response
{
  "status": "ok",
  "model": "gpt-oss-20b-Q4_K_S.gguf",
  "ctx_size": 4096
}
```

---

## SpouÅ¡tÄ›nÃ­ a sprÃ¡va

### start-server.ps1

```powershell
<#
.SYNOPSIS
    SpustÃ­ llama.cpp server s nakonfigurovanÃ½m modelem.
    
.PARAMETER ModelPath
    Cesta k GGUF modelu.
    
.PARAMETER Port
    Port pro server (default: 8001).
    
.PARAMETER GpuLayers
    PoÄet vrstev na GPU (default: 0 pro CPU).
    
.PARAMETER ContextSize
    Velikost context window (default: 4096).
#>

param(
    [string]$ModelPath = "..\models\gpt-oss-20b-Q4_K_S.gguf",
    [int]$Port = 8001,
    [int]$GpuLayers = 0,
    [int]$ContextSize = 4096,
    [int]$Threads = 16
)

# NaÄtenÃ­ konfigurace
$config = Get-Content "config.json" | ConvertFrom-Json

# Override z parametrÅ¯
$modelPath = if ($ModelPath) { $ModelPath } else { $config.model.path }
$port = if ($Port) { $Port } else { $config.server.port }

# SestavenÃ­ pÅ™Ã­kazu
$llamaServer = ".\bin\llama-server.exe"
$args = @(
    "--model", $modelPath,
    "--host", "127.0.0.1",
    "--port", $port,
    "--ctx-size", $ContextSize,
    "--n-gpu-layers", $GpuLayers,
    "--threads", $Threads,
    "-fit", "off"
)

Write-Host "Starting llama.cpp server..." -ForegroundColor Green
Write-Host "Model: $modelPath"
Write-Host "Port: $port"
Write-Host "GPU Layers: $GpuLayers"

# SpuÅ¡tÄ›nÃ­
& $llamaServer @args
```

### SpuÅ¡tÄ›nÃ­

```powershell
# ZÃ¡kladnÃ­ spuÅ¡tÄ›nÃ­
cd llama-cpp-server
.\start-server.ps1

# S parametry
.\start-server.ps1 -Port 8080 -GpuLayers 99 -ContextSize 8192

# Na pozadÃ­
Start-Process -FilePath ".\start-server.ps1" -WindowStyle Minimized
```

### ZastavenÃ­

```powershell
# NajÃ­t a ukonÄit proces
Get-Process llama-server | Stop-Process

# Nebo pomocÃ­ port
netstat -ano | Select-String ":8001"
Stop-Process -Id <PID>
```

---

## Integrace s llama-orchestrator

### AutomatickÃ¡ sprÃ¡va

```powershell
# Inicializace instance
llama-orch init gpt-oss `
  --model "../models/gpt-oss-20b-Q4_K_S.gguf" `
  --port 8001 `
  --context-size 4096 `
  --gpu-layers 0

# SpuÅ¡tÄ›nÃ­ pÅ™es orchestrator
llama-orch up gpt-oss

# Monitoring
llama-orch dashboard
```

### Konfigurace pro orchestrator

```json
// llama-orchestrator/instances/gpt-oss/config.json
{
  "name": "gpt-oss",
  "model": {
    "path": "../../llama-cpp-server/../models/gpt-oss-20b-Q4_K_S.gguf",
    "context_size": 4096,
    "batch_size": 512,
    "threads": 16
  },
  "server": {
    "host": "127.0.0.1",
    "port": 8001,
    "parallel": 4
  },
  "gpu": {
    "layers": 0,
    "backend": "vulkan"
  }
}
```

---

## Troubleshooting

### ÄŒastÃ© problÃ©my

| ProblÃ©m | PÅ™Ã­Äina | Å˜eÅ¡enÃ­ |
|---------|---------|--------|
| Model not found | Å patnÃ¡ cesta | PouÅ¾Ã­t absolutnÃ­ cestu |
| Port in use | JinÃ½ proces | `netstat -ano \| Select-String :8001` |
| Out of memory | VelkÃ½ model | SnÃ­Å¾it context_size |
| Slow inference | CPU mode | OvÄ›Å™it gpu_layers |
| Vulkan error | ChybÃ­ driver | Aktualizovat GPU driver |

### DiagnostickÃ© pÅ™Ã­kazy

```powershell
# OvÄ›Å™enÃ­ modelu
Test-Path "..\models\gpt-oss-20b-Q4_K_S.gguf"

# Velikost modelu
(Get-Item "..\models\gpt-oss-20b-Q4_K_S.gguf").Length / 1GB

# OvÄ›Å™enÃ­ portu
Test-NetConnection -ComputerName localhost -Port 8001

# Health check
Invoke-RestMethod http://localhost:8001/health

# SystÃ©movÃ© zdroje
Get-Process llama-server | Select-Object CPU, WorkingSet64
```

### TestovacÃ­ skript

```powershell
# test-api.ps1
$baseUrl = "http://localhost:8001"

# Health check
Write-Host "Testing health endpoint..."
try {
    $health = Invoke-RestMethod "$baseUrl/health"
    Write-Host "âœ“ Health: $($health.status)" -ForegroundColor Green
} catch {
    Write-Host "âœ— Health check failed" -ForegroundColor Red
    exit 1
}

# Chat completion
Write-Host "Testing chat completion..."
$body = @{
    model = "gpt-oss-20b"
    messages = @(
        @{ role = "user"; content = "Say hello in one word." }
    )
    max_tokens = 10
} | ConvertTo-Json

try {
    $response = Invoke-RestMethod "$baseUrl/v1/chat/completions" `
        -Method POST `
        -ContentType "application/json" `
        -Body $body
    
    $answer = $response.choices[0].message.content
    Write-Host "âœ“ Response: $answer" -ForegroundColor Green
} catch {
    Write-Host "âœ— Chat completion failed: $_" -ForegroundColor Red
}
```

---

## Performance tuning

### Optimalizace pro CPU

```powershell
# MaximÃ¡lnÃ­ CPU vyuÅ¾itÃ­
.\start-server.ps1 `
  -Threads $env:NUMBER_OF_PROCESSORS `
  -GpuLayers 0 `
  -ContextSize 2048  # MenÅ¡Ã­ context = rychlejÅ¡Ã­
```

### Optimalizace pro GPU (standardnÃ­ modely)

```powershell
# MaximÃ¡lnÃ­ GPU offload
.\start-server.ps1 `
  -ModelPath "..\models\llama-3-8b-Q4_K_M.gguf" `
  -GpuLayers 99 `
  -ContextSize 8192
```

### Memory management

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MEMORY REQUIREMENTS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Model Size Estimates (Q4_K quantization):                                  â”‚
â”‚                                                                             â”‚
â”‚  7B params:   ~4 GB model + ~2 GB context = ~6 GB total                    â”‚
â”‚  13B params:  ~8 GB model + ~2 GB context = ~10 GB total                   â”‚
â”‚  20B params:  ~12 GB model + ~3 GB context = ~15 GB total                  â”‚
â”‚  70B params:  ~40 GB model + ~4 GB context = ~44 GB total                  â”‚
â”‚                                                                             â”‚
â”‚  Context memory scales with context_size:                                   â”‚
â”‚  - 2K context: ~0.5 GB                                                      â”‚
â”‚  - 4K context: ~1 GB                                                        â”‚
â”‚  - 8K context: ~2 GB                                                        â”‚
â”‚  - 32K context: ~8 GB                                                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SouvisejÃ­cÃ­ dokumenty

- **Architektura:** [../architecture/ARCHITECTURE.md](../architecture/ARCHITECTURE.md)
- **Llama Orchestrator:** [LLAMA_ORCHESTRATOR.md](LLAMA_ORCHESTRATOR.md)
- **Integrace:** [../architecture/INTEGRATION.md](../architecture/INTEGRATION.md)

---

*Tato dokumentace je souÄÃ¡stÃ­ 4-ÃºrovÅˆovÃ© dokumentaÄnÃ­ struktury projektu MCP Prompt Broker.*
